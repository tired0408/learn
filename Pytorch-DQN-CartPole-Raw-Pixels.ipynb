{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN on CartPole-v1 Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:53.395282Z",
     "start_time": "2019-02-27T13:47:53.315141Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from copy import copy\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T07:07:47.989411Z",
     "start_time": "2019-02-28T07:07:47.979866Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'Dueling_DDQN_Prior_Memory'\n",
    "save_name = 'checkpoints/' + model_name\n",
    "resume = False\n",
    "\n",
    "class Config():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_final = 0.01\n",
    "        self.epsilon_decay = 10\n",
    "        self.TARGET_UPDATE = 200\n",
    "        self.BATCH_SIZE = 256\n",
    "        self.start_from = 512\n",
    "        self.GAMMA = 1\n",
    "        self.dueling = True\n",
    "        self.plot_every = 5\n",
    "        self.lr = 3e-5\n",
    "        self.optim_method = optim.Adam\n",
    "        self.memory_size = 10000\n",
    "        self.conv_layer_settings = [\n",
    "            (3, 8, 5, 2),\n",
    "            (8, 16, 5, 2),\n",
    "            (16, 32, 5, 2),\n",
    "            (32, 32, 5, 2)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:54.664318Z",
     "start_time": "2019-02-27T13:47:53.399720Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ['state', 'action', 'reward', 'next_state', 'terminal'])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=10000):\n",
    "        self.prob_alpha = alpha\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.frame = 1\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "\n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "\n",
    "    def push(self, transition):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0**self.prob_alpha\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        if total < self.capacity:\n",
    "            pos = total\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            prios = self.priorities[:total]\n",
    "            probs = (1 - prios / prios.sum()) / (total - 1)\n",
    "            pos = np.random.choice(total, 1, p=probs)\n",
    "\n",
    "        self.priorities[pos] = max_prio\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        total = len(self.buffer)\n",
    "        prios = self.priorities[:total]\n",
    "        probs = prios / prios.sum()\n",
    "\n",
    "        indices = np.random.choice(total, batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "\n",
    "        # Min of ALL probs, not just sampled probs\n",
    "        prob_min = probs.min()\n",
    "        max_weight = (prob_min*total)**(-beta)\n",
    "\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= max_weight\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float)\n",
    "\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = (prio + 1e-5)**self.prob_alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:54.678740Z",
     "start_time": "2019-02-27T13:47:54.665463Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_params(net):\n",
    "\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal_(m.weight, std=1e-3)\n",
    "            init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, kernel_size, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            input_size, output_size, kernel_size=kernel_size, stride=stride, padding=self.padding)\n",
    "        self.bn = nn.BatchNorm2d(output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "    def size_out(self, size):\n",
    "        return (size - self.kernel_size + self.padding * 2) // self.stride + 1\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, conv_layer_settings, dueling=False):\n",
    "        super(DQN, self).__init__()\n",
    "        self.dueling = dueling\n",
    "\n",
    "        conv_blocks = []\n",
    "        size = np.array([h, w])\n",
    "        for s in conv_layer_settings:\n",
    "            block = ConvBlock(s[0], s[1], s[2], s[3])\n",
    "            conv_blocks.append(block)\n",
    "            size = block.size_out(size)\n",
    "        self.conv_step = nn.Sequential(*conv_blocks)\n",
    "        linear_input_size = size[0] * size[1] * conv_layer_settings[-1][1]\n",
    "\n",
    "        if self.dueling:\n",
    "            self.adv = nn.Linear(linear_input_size, 2)\n",
    "            self.val = nn.Linear(linear_input_size, 1)\n",
    "        else:\n",
    "            self.head = nn.Linear(linear_input_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_step(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        if self.dueling:\n",
    "            adv = F.relu(self.adv(x))\n",
    "            val = F.relu(self.val(x))\n",
    "            return val + adv - val.mean()\n",
    "        else:\n",
    "            return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:56.896504Z",
     "start_time": "2019-02-27T13:47:54.680191Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Event, Thread\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.CenterCrop((250, 500)),\n",
    "                    T.Resize(64),\n",
    "                    T.Grayscale(),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "class RenderThread(Thread):\n",
    "    # Usage:\n",
    "    # 0. call env.step() or env.reset() to update env state\n",
    "    # 1. call begin_render() to schedule a rendering task (non-blocking)\n",
    "    # 2. call get_screen() to get the lastest scheduled result (block main thread if rendering not done)\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(RenderThread, self).__init__(target=self.render)\n",
    "        self._stop_event = Event()\n",
    "        self._state_event = Event()\n",
    "        self._render_event = Event()\n",
    "        self.env = env\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop_event.set()\n",
    "        self._state_event.set()\n",
    "\n",
    "    def stopped(self):\n",
    "        return self._stop_event.is_set()\n",
    "\n",
    "    def begin_render(self):\n",
    "        self._state_event.set()\n",
    "\n",
    "    def get_screen(self):\n",
    "        self._render_event.wait()\n",
    "        self._render_event.clear()\n",
    "        return self.screen\n",
    "\n",
    "    def render(self):\n",
    "        while not self.stopped():\n",
    "            self._state_event.wait()\n",
    "            self._state_event.clear()\n",
    "\n",
    "            self.screen = self.env.render(\n",
    "                mode='rgb_array').transpose((2, 0, 1))\n",
    "            self.screen = np.ascontiguousarray(\n",
    "                self.screen, dtype=np.float32) / 255\n",
    "            self.screen = torch.from_numpy(self.screen)\n",
    "            self.screen = resize(self.screen).unsqueeze(0).to(device)\n",
    "            self._render_event.set()\n",
    "\n",
    "\n",
    "# A simple test\n",
    "renderer = RenderThread(env)\n",
    "renderer.start()\n",
    "\n",
    "env.reset()\n",
    "renderer.begin_render()\n",
    "screen = renderer.get_screen()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen.cpu().squeeze(0).permute(\n",
    "    1, 2, 0).numpy().squeeze(), cmap='gray')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()\n",
    "renderer.stop()\n",
    "renderer.join()\n",
    "\n",
    "_, _, screen_height, screen_width = screen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:56.912375Z",
     "start_time": "2019-02-27T13:47:56.898118Z"
    }
   },
   "outputs": [],
   "source": [
    "class History():\n",
    "\n",
    "    def __init__(self, plot_size=300, plot_every=5):\n",
    "        self.plot_size = plot_size\n",
    "        self.episode_durations = deque([], self.plot_size)\n",
    "        self.means = deque([], self.plot_size)\n",
    "        self.episode_loss = deque([], self.plot_size)\n",
    "        self.indexes = deque([], self.plot_size)\n",
    "        self.step_loss = []\n",
    "        self.step_eps = []\n",
    "        self.peak_reward = 0\n",
    "        self.peak_mean = 0\n",
    "        self.moving_avg = 0\n",
    "        self.step_count = 0\n",
    "        self.total_episode = 0\n",
    "        self.plot_every = plot_every\n",
    "\n",
    "    def update(self, t, episode_loss):\n",
    "        self.episode_durations.append(t + 1)\n",
    "        self.episode_loss.append(episode_loss / (t + 1))\n",
    "        self.indexes.append(self.total_episode)\n",
    "        if t + 1 > self.peak_reward:\n",
    "            self.peak_reward = t + 1\n",
    "        if len(self.episode_durations) >= 100:\n",
    "            self.means.append(sum(list(self.episode_durations)[-100:]) / 100)\n",
    "        else:\n",
    "            self.moving_avg = self.moving_avg + \\\n",
    "                (t - self.moving_avg) / (self.total_episode + 1)\n",
    "            self.means.append(self.moving_avg)\n",
    "        if self.means[-1] > self.peak_mean:\n",
    "            self.peak_mean = self.means[-1]\n",
    "\n",
    "        if self.total_episode % self.plot_every == 0:\n",
    "            self.plot()\n",
    "\n",
    "    def plot(self):\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        f, (ax1, ax3) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        ax1.plot(self.indexes, self.episode_durations)\n",
    "        ax1.plot(self.indexes, self.means)\n",
    "        ax1.axhline(self.peak_reward, color='g')\n",
    "        ax1.axhline(self.peak_mean, color='g')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(self.indexes, self.episode_loss, 'r')\n",
    "\n",
    "        ax4 = ax3.twinx()\n",
    "        total_step = len(self.step_loss)\n",
    "        sample_rate = total_step // self.plot_size if total_step > (\n",
    "            self.plot_size * 10) else 1\n",
    "        ax3.set_title('total: {0}'.format(total_step))\n",
    "        ax3.plot(self.step_eps[::sample_rate], 'g')\n",
    "        ax4.plot(self.step_loss[::sample_rate], 'b')\n",
    "\n",
    "        plt.pause(0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Hyperparameters\n",
    "Run this cell if you want to start a fresh new training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:57.048244Z",
     "start_time": "2019-02-27T13:47:56.990560Z"
    }
   },
   "outputs": [],
   "source": [
    "# comment this out if a checkpoint is available\n",
    "# load_name = 'checkpoints/checkpoint'\n",
    "\n",
    "# Init network\n",
    "if resume:\n",
    "    print('loading checkpoint...')\n",
    "    with open(save_name + '.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        history = data['history']\n",
    "        config = data['config']\n",
    "\n",
    "    checkpoint = torch.load(save_name + '.pt')\n",
    "\n",
    "    policy_net = DQN(screen_height, screen_width,\n",
    "                     config.conv_layer_settings).to(device)\n",
    "    target_net = DQN(screen_height, screen_width,\n",
    "                     config.conv_layer_settings).to(device)\n",
    "    policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "    target_net.load_state_dict(checkpoint['target_net'])\n",
    "\n",
    "    optimizer = config.optim_method(policy_net.parameters(), lr=config.lr)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "else:\n",
    "    print('fresh start...')\n",
    "    history = History()\n",
    "    config = Config()\n",
    "\n",
    "    policy_net = DQN(screen_height, screen_width,\n",
    "                     config.conv_layer_settings, config.dueling).to(device)\n",
    "    target_net = DQN(screen_height, screen_width,\n",
    "                     config.conv_layer_settings, config.dueling).to(device)\n",
    "    init_params(policy_net)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = config.optim_method(policy_net.parameters(), lr=config.lr)\n",
    "\n",
    "memory = ReplayMemory(config.memory_size)\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T13:47:57.087923Z",
     "start_time": "2019-02-27T13:47:57.050377Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model(step):\n",
    "    if len(memory) < config.start_from:\n",
    "        return 0\n",
    "\n",
    "    if step % config.TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # Sample memory as a batch\n",
    "    samples, ids, weights = memory.sample(config.BATCH_SIZE)\n",
    "    batch = Transition(*zip(*samples))\n",
    "\n",
    "    # A tensor cannot be None, so strip out terminal states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat(\n",
    "        [s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Bellman's Equation\n",
    "    with torch.no_grad():\n",
    "        online_Q = policy_net(non_final_next_states)\n",
    "        target_Q = target_net(non_final_next_states)\n",
    "        next_Q = torch.zeros(config.BATCH_SIZE, device=device)\n",
    "        next_Q[non_final_mask] = target_Q.gather(\n",
    "            1, online_Q.max(1)[1].detach().unsqueeze(1)).squeeze(1)\n",
    "        target_Q = next_Q * config.GAMMA + reward_batch\n",
    "\n",
    "    # Compute loss\n",
    "    policy_net.train()\n",
    "    current_Q = policy_net(state_batch).gather(1, action_batch)\n",
    "    diff = current_Q.squeeze() - target_Q\n",
    "    loss = (0.5 * (diff * diff) * weights).mean()\n",
    "\n",
    "    # Update memory\n",
    "    delta = diff.abs().detach().cpu().numpy().tolist()\n",
    "    memory.update_priorities(ids, delta)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def epsilon_by_frame(frame_idx):\n",
    "    return config.epsilon_final + \\\n",
    "        (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
    "\n",
    "\n",
    "def select_action(state, eps):\n",
    "    sample = random.random()\n",
    "    if sample > eps:\n",
    "        policy_net.eval()\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop\n",
    "This cell can be run multiple times without rerun the above cells.\n",
    "\n",
    "Just change the loop size and you can continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T03:52:40.822542Z",
     "start_time": "2019-02-27T13:47:57.088968Z"
    }
   },
   "outputs": [],
   "source": [
    "renderer = RenderThread(env)\n",
    "renderer.start()\n",
    "\n",
    "for i_episode in range(10000):\n",
    "    history.total_episode += 1\n",
    "\n",
    "    env.reset()\n",
    "    renderer.begin_render()\n",
    "\n",
    "    init_screen = renderer.get_screen()\n",
    "    screens = deque([init_screen] * 3, 3)\n",
    "    state = torch.cat(list(screens), dim=1)\n",
    "    avg_loss = 0\n",
    "\n",
    "    for t in count():\n",
    "        history.step_count += 1\n",
    "\n",
    "        # Select and perform an action\n",
    "        eps = epsilon_by_frame(history.total_episode)\n",
    "        action = select_action(state, eps)\n",
    "        history.step_eps.append(eps)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Render the step in another thread\n",
    "        renderer.begin_render()\n",
    "\n",
    "        # Do optimization in main thread\n",
    "        loss = optimize_model(history.step_count)\n",
    "        avg_loss += loss\n",
    "        history.step_loss.append(loss)\n",
    "\n",
    "        # Render the next_state and remember it\n",
    "        screens.append(renderer.get_screen())\n",
    "        next_state = torch.cat(list(screens), dim=1) if not done else None\n",
    "        memory.push(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            history.update(t, avg_loss)\n",
    "            break\n",
    "\n",
    "renderer.stop()\n",
    "renderer.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T07:09:10.538262Z",
     "start_time": "2019-02-28T07:09:03.916105Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'policy_net': policy_net.state_dict(),\n",
    "    'target_net': target_net.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, save_name + '.pt')\n",
    "\n",
    "with open(save_name + '.pickle', 'wb') as f:\n",
    "    pickle.dump({'history': history, 'config': config},\n",
    "                f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T11:03:48.917258Z",
     "start_time": "2019-02-23T11:03:48.891469Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(save_name + '.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    history = data['history']\n",
    "    config = data['config']\n",
    "\n",
    "checkpoint = torch.load(save_name + '.pt')\n",
    "policy_net = DQN(screen_height, screen_width,\n",
    "                 config.conv_layer_settings, config.dueling).to(device)\n",
    "policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "policy_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T10:39:04.260900Z",
     "start_time": "2019-02-23T10:38:59.345035Z"
    }
   },
   "outputs": [],
   "source": [
    "renderer = RenderThread(env)\n",
    "renderer.start()\n",
    "\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    renderer.begin_render()\n",
    "\n",
    "    init_screen = renderer.get_screen()\n",
    "    screens = deque([init_screen] * 3, 3)\n",
    "    state = torch.cat(list(screens), dim=1)\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Select and perform an action\n",
    "        action = select_action(state, 0)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        total_reward += reward\n",
    "\n",
    "        # Render the step in another thread\n",
    "        renderer.begin_render()\n",
    "\n",
    "        # Render the next_state and remember it\n",
    "        screens.append(renderer.get_screen())\n",
    "        next_state = torch.cat(list(screens), dim=1) if not done else None\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print('total reward:', total_reward)\n",
    "            break\n",
    "\n",
    "renderer.stop()\n",
    "renderer.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
